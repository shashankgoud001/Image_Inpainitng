{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-04-15T09:05:51.244671Z","iopub.status.busy":"2023-04-15T09:05:51.244248Z","iopub.status.idle":"2023-04-15T09:05:56.535412Z","shell.execute_reply":"2023-04-15T09:05:56.534665Z","shell.execute_reply.started":"2023-04-15T09:05:51.244562Z"},"trusted":true},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras import backend as K\n","from tensorflow.keras import layers\n","from tensorflow.keras.applications.vgg16 import VGG16,preprocess_input\n","from tensorflow.python.keras.utils import conv_utils\n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import os\n","from tqdm import tqdm\n","import pickle\n","\n","\n","from random import randint, seed\n","import itertools\n","import cv2\n","\n","import math\n","import random\n","from PIL import Image, ImageDraw\n","\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import cv2\n"]},{"cell_type":"markdown","metadata":{},"source":["## Notebook "]},{"cell_type":"markdown","metadata":{},"source":["#### Codes are mainly from : [Partial Convolution Keras](https://github.com/MathiasGruber/PConv-Keras)\n","\n","\n","#### Purpose : \n","\n","#### 1. Testing some properties about texture & content synthesis for image inpainting task\n","\n","#### 2. This notebook trains 2 models to test the idea from [Instance Normalization: The Missing Ingredient for Fast Stylization](https://arxiv.org/abs/1607.08022)\n","\n","#### 3. And further do some experiment about difference between In and Bn, better understanding the idea from  [A Neural Algorithm of Artistic Style](https://arxiv.org/abs/1508.06576) and [Perceptual Losses for Real-Time Style Transfer and Super-Resolution](https://arxiv.org/abs/1603.08155)"]},{"cell_type":"markdown","metadata":{},"source":["# Model"]},{"cell_type":"markdown","metadata":{},"source":["## Partial Convolution"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-04-15T09:05:56.537312Z","iopub.status.busy":"2023-04-15T09:05:56.537047Z","iopub.status.idle":"2023-04-15T09:05:56.557307Z","shell.execute_reply":"2023-04-15T09:05:56.556604Z","shell.execute_reply.started":"2023-04-15T09:05:56.537275Z"},"trusted":true},"outputs":[],"source":["class PConv2D(tf.keras.layers.Conv2D):\n","    def __init__(self, *args, n_channels=3, mono=False, **kwargs):\n","        super().__init__(*args, **kwargs)\n","        self.input_spec = [tf.keras.layers.InputSpec(ndim=4), tf.keras.layers.InputSpec(ndim=4)]\n","\n","    def build(self, input_shape):        \n","        \"\"\"Adapted from original _Conv() layer of Keras        \n","        param input_shape: list of dimensions for [img, mask]\n","        \"\"\"\n","        \n","        if self.data_format == 'channels_first':\n","            channel_axis = 1\n","        else:\n","            channel_axis = -1\n","            \n","        if input_shape[0][channel_axis] is None:\n","            raise ValueError('The channel dimension of the inputs should be defined. Found `None`.')\n","            \n","        self.input_dim = input_shape[0][channel_axis]\n","        \n","        # Image kernel\n","        kernel_shape = self.kernel_size + (self.input_dim, self.filters)\n","        self.kernel = self.add_weight(shape=kernel_shape,\n","                                      initializer=self.kernel_initializer,\n","                                      name='img_kernel',\n","                                      regularizer=self.kernel_regularizer,\n","                                      constraint=self.kernel_constraint)\n","        # Mask kernel\n","        self.kernel_mask = K.ones(shape=self.kernel_size + (self.input_dim, self.filters))\n","\n","        # Calculate padding size to achieve zero-padding\n","        self.pconv_padding = (\n","            (int((self.kernel_size[0]-1)/2), int((self.kernel_size[0]-1)/2)), \n","            (int((self.kernel_size[0]-1)/2), int((self.kernel_size[0]-1)/2)), \n","        )\n","\n","        # Window size - used for normalization\n","        self.window_size = self.kernel_size[0] * self.kernel_size[1]\n","        \n","        if self.use_bias:\n","            self.bias = self.add_weight(shape=(self.filters,),\n","                                        initializer=self.bias_initializer,\n","                                        name='bias',\n","                                        regularizer=self.bias_regularizer,\n","                                        constraint=self.bias_constraint)\n","        else:\n","            self.bias = None\n","        self.built = True\n","\n","    def call(self, inputs, mask=None):\n","\n","        # Padding done explicitly so that padding becomes part of the masked partial convolution\n","        images = K.spatial_2d_padding(inputs[0], self.pconv_padding, self.data_format)\n","        masks = K.spatial_2d_padding(inputs[1], self.pconv_padding, self.data_format)\n","\n","        # Apply convolutions to mask\n","        mask_output = K.conv2d(\n","            masks, self.kernel_mask, \n","            strides=self.strides,\n","            padding='valid',\n","            data_format=self.data_format,\n","            dilation_rate=self.dilation_rate\n","        )\n","\n","        # Apply convolutions to image\n","        img_output = K.conv2d(\n","            (images*masks), self.kernel, \n","            strides=self.strides,\n","            padding='valid',\n","            data_format=self.data_format,\n","            dilation_rate=self.dilation_rate\n","        )        \n","\n","        # Calculate the mask ratio on each pixel in the output mask\n","        mask_ratio = self.window_size / (mask_output + 1e-8)\n","\n","        # Clip output to be between 0 and 1\n","        mask_output = K.clip(mask_output, 0, 1)\n","\n","        # Remove ratio values where there are holes\n","        mask_ratio = mask_ratio * mask_output\n","\n","        # Normalize iamge output\n","        img_output = img_output * mask_ratio\n","\n","        # Apply bias only to the image (if chosen to do so)\n","        if self.use_bias:\n","            img_output = K.bias_add(\n","                img_output,\n","                self.bias,\n","                data_format=self.data_format)\n","        \n","        # Apply activations on the image\n","        if self.activation is not None:\n","            img_output = self.activation(img_output)\n","            \n","        return [img_output, mask_output]\n","    \n","    def compute_output_shape(self, input_shape):\n","        space = input_shape[0][1:-1]\n","        new_space = []\n","        for i in range(len(space)):\n","            new_dim = conv_utils.conv_output_length(\n","                space[i],\n","                self.kernel_size[i],\n","                padding='same',\n","                stride=self.strides[i],\n","                dilation=self.dilation_rate[i])\n","            new_space.append(new_dim)\n","        new_shape = (input_shape[0][0],) + tuple(new_space) + (self.filters,)\n","        return [new_shape, new_shape]"]},{"cell_type":"markdown","metadata":{},"source":["## Instance Normalization"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-04-15T09:05:56.559491Z","iopub.status.busy":"2023-04-15T09:05:56.559227Z","iopub.status.idle":"2023-04-15T09:05:56.569194Z","shell.execute_reply":"2023-04-15T09:05:56.568492Z","shell.execute_reply.started":"2023-04-15T09:05:56.559457Z"},"trusted":true},"outputs":[],"source":["class InstanceNormalization(tf.keras.layers.Layer):\n","    def __init__(self, epsilon=1e-5):\n","        super(InstanceNormalization, self).__init__()\n","        self.epsilon = epsilon\n","        \n","    def build(self, input_shape):\n","        self.scale = self.add_weight(\n","            name='scale',\n","            shape=input_shape[-1:],\n","            initializer=tf.random_normal_initializer(1., 0.02),trainable=True)\n","\n","        self.offset = self.add_weight(\n","            name='offset',\n","            shape=input_shape[-1:],\n","            initializer='zeros',\n","            trainable=True)\n","        \n","    def call(self, x):\n","        mean, variance = tf.nn.moments(x, axes=[1, 2], keepdims=True)\n","        inv = tf.math.rsqrt(variance + self.epsilon)\n","        normalized = (x - mean) * inv\n","        return self.scale * normalized + self.offset"]},{"cell_type":"markdown","metadata":{},"source":["## Generator UNET "]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-04-15T09:05:56.572840Z","iopub.status.busy":"2023-04-15T09:05:56.572621Z","iopub.status.idle":"2023-04-15T09:05:56.588117Z","shell.execute_reply":"2023-04-15T09:05:56.587451Z","shell.execute_reply.started":"2023-04-15T09:05:56.572810Z"},"trusted":true},"outputs":[],"source":["    def generator(norm_type='Bn',train=True):      \n","\n","        # INPUTS\n","        inputs_img = tf.keras.layers.Input((None, None, 3), name='inputs_img')\n","        inputs_mask = tf.keras.layers.Input((None, None, 3), name='inputs_mask')\n","        \n","        \n","        def encoder_layer(img_in, mask_in, filters, kernel_size,norm_type,norm=True):\n","            conv, mask = PConv2D(filters, kernel_size, strides=2, padding='same')([img_in, mask_in])\n","            if norm:\n","                if norm_type=='In':\n","                    conv=InstanceNormalization()(conv)\n","                else:\n","                    conv=tf.keras.layers.BatchNormalization()(conv,training=train)\n","                    \n","            conv = tf.keras.layers.ReLU()(conv)\n","            return conv, mask\n","\n","        \n","        e_conv1, e_mask1 = encoder_layer(inputs_img, inputs_mask, 64, 7, norm_type,norm=False)\n","        e_conv2, e_mask2 = encoder_layer(e_conv1, e_mask1, 128, 5,norm_type)\n","        e_conv3, e_mask3 = encoder_layer(e_conv2, e_mask2, 256, 5,norm_type)\n","        e_conv4, e_mask4 = encoder_layer(e_conv3, e_mask3, 512, 3,norm_type)\n","        e_conv5, e_mask5 = encoder_layer(e_conv4, e_mask4, 512, 3,norm_type)\n","        e_conv6, e_mask6 = encoder_layer(e_conv5, e_mask5, 512, 3,norm_type)\n","        e_conv7, e_mask7 = encoder_layer(e_conv6, e_mask6, 512, 3,norm_type)\n","        e_conv8, e_mask8 = encoder_layer(e_conv7, e_mask7, 512, 3,norm_type)\n","        \n","        \n","        def decoder_layer(img_in, mask_in, e_conv, e_mask, filters, kernel_size,norm_type, norm=True):\n","            up_img = tf.keras.layers.UpSampling2D(size=(2, 2))(img_in)\n","            up_mask = tf.keras.layers.UpSampling2D(size=(2, 2))(mask_in)\n","            concat_img = tf.keras.layers.Concatenate(axis=-1)([e_conv, up_img])\n","            concat_mask = tf.keras.layers.Concatenate(axis=-1)([e_mask, up_mask])\n","            \n","            conv, mask = PConv2D(filters, kernel_size, padding='same')([concat_img, concat_mask])\n","            \n","            if norm:\n","                if norm_type=='In':\n","                    conv=InstanceNormalization()(conv)\n","                else:\n","                    conv=tf.keras.layers.BatchNormalization()(conv,training=train)\n","            conv = tf.keras.layers.LeakyReLU(alpha=0.2)(conv)\n","            return conv, mask\n","            \n","        d_conv9, d_mask9 = decoder_layer(e_conv8, e_mask8, e_conv7, e_mask7, 512, 3,norm_type)\n","        d_conv10, d_mask10 = decoder_layer(d_conv9, d_mask9, e_conv6, e_mask6, 512, 3,norm_type)\n","        d_conv11, d_mask11 = decoder_layer(d_conv10, d_mask10, e_conv5, e_mask5, 512, 3,norm_type)\n","        d_conv12, d_mask12 = decoder_layer(d_conv11, d_mask11, e_conv4, e_mask4, 512, 3,norm_type)\n","        d_conv13, d_mask13 = decoder_layer(d_conv12, d_mask12, e_conv3, e_mask3, 256, 3,norm_type)\n","        d_conv14, d_mask14 = decoder_layer(d_conv13, d_mask13, e_conv2, e_mask2, 128, 3,norm_type)\n","        d_conv15, d_mask15 = decoder_layer(d_conv14, d_mask14, e_conv1, e_mask1, 64, 3,norm_type)\n","        d_conv16, d_mask16 = decoder_layer(d_conv15, d_mask15, inputs_img, inputs_mask, 3, 3, norm_type,norm=False)\n","        outputs = tf.keras.layers.Conv2D(3, 1, activation = 'sigmoid', name='outputs_img')(d_conv16)\n","        \n","        # Setup the model inputs / outputs\n","        model = tf.keras.Model(inputs=[inputs_img, inputs_mask], outputs=outputs)\n","\n","        return model"]},{"cell_type":"markdown","metadata":{},"source":["## VGG Extractor"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-04-15T09:05:56.590213Z","iopub.status.busy":"2023-04-15T09:05:56.589746Z","iopub.status.idle":"2023-04-15T09:05:56.602920Z","shell.execute_reply":"2023-04-15T09:05:56.602188Z","shell.execute_reply.started":"2023-04-15T09:05:56.590179Z"},"trusted":true},"outputs":[],"source":["def VGG():\n","\n","    #pool1, pool2 and pool3 for both perceptual loss & style loss\n","    \n","    vgg16=VGG16(include_top=False,weights=None)\n","    vgg16.load_weights('/kaggle/input/vgg16-weights/pytorch_to_keras_vgg16/pytorch_to_keras_vgg16.h5',by_name=True)\n","    vgg16.trainable=False\n","    \n","    layer_names=['block1_pool','block2_pool','block3_pool']\n","    \n","    outputs = [vgg16.get_layer(name).output for name in layer_names]\n","    \n","    return tf.keras.Model([vgg16.input], outputs)"]},{"cell_type":"markdown","metadata":{},"source":["# Training"]},{"cell_type":"markdown","metadata":{},"source":["## Prepare Data"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2023-04-15T09:10:43.946893Z","iopub.status.busy":"2023-04-15T09:10:43.946593Z","iopub.status.idle":"2023-04-15T09:10:43.952066Z","shell.execute_reply":"2023-04-15T09:10:43.951031Z","shell.execute_reply.started":"2023-04-15T09:10:43.946860Z"},"trusted":true},"outputs":[],"source":["img_size=256\n","\n","batch_size=16\n","\n","lr=1e-4\n","\n","epochs=20\n","\n","AUTOTUNE = tf.data.experimental.AUTOTUNE\n","\n","fine_tune=True #fine tune\n","frozen_layers=[5,8,11,14,17,20,23]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!mkdir ~/.kaggle\n","!mkdir submission\n","!touch ~/.kaggle/kaggle.json\n","\n","api_token = {\"username\":\"spartan1001\",\"key\":\"b410e5c12a85cbb74fab8586d2e648b7\"}\n","\n","import json\n","\n","with open('/root/.kaggle/kaggle.json', 'w') as file:\n","    json.dump(api_token, file)\n","\n","!chmod 600 ~/.kaggle/kaggle.json\n","!kaggle competitions download -c photo-reconstruction\n","!unzip ./photo-reconstruction.zip"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-04-15T09:06:18.098972Z","iopub.status.busy":"2023-04-15T09:06:18.098654Z","iopub.status.idle":"2023-04-15T09:06:20.146574Z","shell.execute_reply":"2023-04-15T09:06:20.145481Z","shell.execute_reply.started":"2023-04-15T09:06:18.098928Z"},"trusted":true},"outputs":[],"source":["!mkdir Dataset/newtrain_masked\n","!mkdir Dataset/newtrain_unmasked"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-04-15T09:06:20.149689Z","iopub.status.busy":"2023-04-15T09:06:20.149101Z","iopub.status.idle":"2023-04-15T09:06:20.155116Z","shell.execute_reply":"2023-04-15T09:06:20.154422Z","shell.execute_reply.started":"2023-04-15T09:06:20.149643Z"},"trusted":true},"outputs":[],"source":["\n","animals = ['Cat','Dog','Elephant','Tiger']\n","pth = 'Dataset/Training_Data/Cat/Unmasked_Train/Cat-Train (1).jpeg'\n","new_pth_masked = 'Dataset/newtrain_masked/'\n","new_pth_unmasked = 'Dataset/newtrain_unmasked/'\n","\n","  \n","\n","\n","# shape1 = [(158, 79), (158+75, 79+75)]\n","# shape2 = [(158,115),(158+75,115+75)]\n","# shape = [(x1,y1),(x1+75,y1+75)]\n","\n","# creating new Image object\n","# img = Image.open(r\"Cat-Train (1).jpeg\")\n","# img = Image.new(\"RGB\", (w, h),color = (255, 255, 255))\n","\n","# create  rectangleimage\n","\n","\n","# img.show()"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-04-15T09:06:20.158913Z","iopub.status.busy":"2023-04-15T09:06:20.158650Z","iopub.status.idle":"2023-04-15T09:07:33.589329Z","shell.execute_reply":"2023-04-15T09:07:33.588571Z","shell.execute_reply.started":"2023-04-15T09:06:20.158866Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["foundmasked_info.csv\n","foundmasked_info.csv\n","foundmasked_info.csv\n","foundmasked_info.csv\n","                                                     pth  \\\n","0      Dataset/Training_Data/Cat/Unmasked_Train/Cat-T...   \n","1      Dataset/Training_Data/Cat/Unmasked_Train/Cat-T...   \n","2      Dataset/Training_Data/Cat/Unmasked_Train/Cat-T...   \n","3      Dataset/Training_Data/Cat/Unmasked_Train/Cat-T...   \n","4      Dataset/Training_Data/Cat/Unmasked_Train/Cat-T...   \n","...                                                  ...   \n","20995                Dataset/newtrain_unmasked/1987.jpeg   \n","20996                Dataset/newtrain_unmasked/3632.jpeg   \n","20997               Dataset/newtrain_unmasked/10485.jpeg   \n","20998                Dataset/newtrain_unmasked/4788.jpeg   \n","20999                Dataset/newtrain_unmasked/2108.jpeg   \n","\n","                                                pth_mask  \n","0      Dataset/Training_Data/Cat/Masked_Train/Cat-Tra...  \n","1      Dataset/Training_Data/Cat/Masked_Train/Cat-Tra...  \n","2      Dataset/Training_Data/Cat/Masked_Train/Cat-Tra...  \n","3      Dataset/Training_Data/Cat/Masked_Train/Cat-Tra...  \n","4      Dataset/Training_Data/Cat/Masked_Train/Cat-Tra...  \n","...                                                  ...  \n","20995                  Dataset/newtrain_masked/1987.jpeg  \n","20996                  Dataset/newtrain_masked/3632.jpeg  \n","20997                 Dataset/newtrain_masked/10485.jpeg  \n","20998                  Dataset/newtrain_masked/4788.jpeg  \n","20999                  Dataset/newtrain_masked/2108.jpeg  \n","\n","[21000 rows x 2 columns]\n"]}],"source":["pth='Dataset/Training_Data/Cat/Unmasked_Train'\n","pth_mask='Dataset/Training_Data/Cat/Masked_Train'\n","\n","new_pth_masked = 'Dataset/newtrain_masked'\n","new_pth_unmasked = 'Dataset/newtrain_unmasked'\n","#train\n","train_folder = [[],[],[],[],[]]\n","train_mask_folder = [[],[],[],[],[]]\n","w, h = 256, 256\n","# for i in os.listdir(f'{pth}'):\n","#     print(i)\n","animals = ['Cat','Dog','Elephant','Tiger']\n","i = 0\n","counter = 1\n","for animal in animals:\n","    pth = 'Dataset/Training_Data/'+animal+'/Unmasked_Train'\n","    pth_mask = 'Dataset/Training_Data/'+animal+'/Masked_Train'\n","    for filename in os.listdir(f'{pth}'):\n","        if filename[-3:]==\"csv\":\n","            print(\"found\"+filename)\n","        train_folder[i].append(filename)\n","        x1 = random.randint(0, w-75)\n","        y1 = random.randint(0, h-75)\n","        x2 = random.randint(0, w-75)\n","        y2 = random.randint(0, h-75)\n","        \n","        shape1 = [( x1,y1), (x1+75, y1+75)]\n","        shape2 = [(x2,y2),(x2+75,y2+75)]\n","        \n","        img = Image.open(pth+\"/\"+filename)\n","        img.save(new_pth_unmasked+\"/\"+str(counter)+\".jpeg\")\n","        \n","        img1 = ImageDraw.Draw(img)  \n","        img1.rectangle(shape1, fill =\"#000000\")\n","        img1.rectangle(shape2,fill=\"#000000\")\n","        img.save(new_pth_masked+\"/\"+str(counter)+\".jpeg\")\n","        \n","        counter = counter + 1\n","        \n","        x1 = random.randint(0, w-75)\n","        y1 = random.randint(0, h-75)\n","        x2 = random.randint(0, w-75)\n","        y2 = random.randint(0, h-75)\n","        \n","        shape1 = [( x1,y1), (x1+75, y1+75)]\n","        shape2 = [(x2,y2),(x2+75,y2+75)]\n","        \n","        img = Image.open(pth+\"/\"+filename)\n","        img.save(new_pth_unmasked+\"/\"+str(counter)+\".jpeg\")\n","        \n","        img1 = ImageDraw.Draw(img)  \n","        img1.rectangle(shape1, fill =\"#000000\")\n","        img1.rectangle(shape2,fill=\"#000000\")\n","        img.save(new_pth_masked+\"/\"+str(counter)+\".jpeg\")\n","        counter = counter+1\n","        \n","    for filename in os.listdir(f'{pth_mask}'):\n","        if filename[-3:]==\"csv\":\n","            print(\"found\"+filename)\n","            continue\n","        train_mask_folder[i].append(filename)\n","    i = i +1\n","for filename in os.listdir(f'{new_pth_unmasked}'):\n","    train_folder[i].append(filename)\n","    train_mask_folder[i].append(filename)\n","i = 0\n","animals = ['Cat','Dog','Elephant','Tiger','new']\n","for animal in animals:\n","    if animal==\"new\":\n","        pth = new_pth_unmasked\n","        pth_mask = new_pth_masked\n","    else:\n","        pth = 'Dataset/Training_Data/'+animal+'/Unmasked_Train'\n","        pth_mask = 'Dataset/Training_Data/'+animal+'/Masked_Train'\n","    if i==0:\n","        df_train = pd.DataFrame(np.vstack([train_folder[i],train_mask_folder[i]]).T,columns=['pth','pth_mask'])\n","        df_train['pth']=df_train['pth'].apply(lambda x: os.path.join(f'{pth}/{x}'))\n","        df_train['pth_mask']=df_train['pth_mask'].apply(lambda x: os.path.join(f'{pth_mask}/{x}'))\n","    else:\n","        temp_train = pd.DataFrame(np.vstack([train_folder[i],train_mask_folder[i]]).T,columns=['pth','pth_mask'])\n","        temp_train['pth']=temp_train['pth'].apply(lambda x: os.path.join(f'{pth}/{x}'))\n","        temp_train['pth_mask']=temp_train['pth_mask'].apply(lambda x: os.path.join(f'{pth_mask}/{x}'))\n","        df_train = pd.concat([df_train, temp_train], ignore_index=True)\n","    i = i+1\n","\n","        \n","# for i in os.listdir(f'{pth_mask}'):\n","#     print(i)\n","#     train_folder.append(i)\n","\n","#     train_folder += i\n","# train_folder = (os.listdir(f'{pth}'))\n","# print(len(train_folder[0]))\n","# print(len(train_mask_folder[0]))\n","# train_mask_folder=sorted(os.listdir(f'{pth_mask}/train'))\n","# df_train=pd.DataFrame(np.vstack([train_folder,train_mask_folder]).T,columns=['pth','pth_mask'])\n","# df_train['pth']=df_train['pth'].apply(lambda x: os.path.join(f'{pth}/train/{x}'))\n","# df_train['pth_mask']=df_train['pth_mask'].apply(lambda x: os.path.join(f'{pth_mask}/train/{x}'))\n","print(df_train)\n","\n","#val\n","# val_folder=sorted(os.listdir(f'{pth}/val'))\n","# val_mask_folder=sorted(os.listdir(f'{pth_mask}/val'))\n","# df_val=pd.DataFrame(np.vstack([val_folder,val_mask_folder]).T,columns=['pth','pth_mask'])\n","# df_val['pth']=df_val['pth'].apply(lambda x: os.path.join(f'{pth}/val/{x}'))\n","# df_val['pth_mask']=df_val['pth_mask'].apply(lambda x: os.path.join(f'{pth_mask}/val/{x}'))"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2023-04-15T09:10:59.060415Z","iopub.status.busy":"2023-04-15T09:10:59.059670Z","iopub.status.idle":"2023-04-15T09:10:59.066534Z","shell.execute_reply":"2023-04-15T09:10:59.065713Z","shell.execute_reply.started":"2023-04-15T09:10:59.060377Z"},"trusted":true},"outputs":[],"source":["def get_image(path,path_mask):\n","    image = tf.image.decode_jpeg(tf.io.read_file(path), channels=3)\n","    image=tf.cast(tf.image.resize(image,(img_size,img_size)),'float32')\n","    image=image/255.\n","    \n","    mask = tf.image.decode_jpeg(tf.io.read_file(path_mask), channels=3)\n","    mask=tf.cast(tf.image.resize(mask,(img_size,img_size)),'float32')\n","    mask=mask/255.\n","    return image,mask"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2023-04-15T09:11:00.777275Z","iopub.status.busy":"2023-04-15T09:11:00.776999Z","iopub.status.idle":"2023-04-15T09:11:00.811020Z","shell.execute_reply":"2023-04-15T09:11:00.810284Z","shell.execute_reply.started":"2023-04-15T09:11:00.777244Z"},"trusted":true},"outputs":[],"source":["#all 0~255\n","ds_train=tf.data.Dataset.from_tensor_slices((df_train['pth'],df_train['pth_mask'])).map(get_image,num_parallel_calls=AUTOTUNE).\\\n","                        shuffle(256).batch(batch_size,drop_remainder=True)\n","\n","# ds_val=tf.data.Dataset.from_tensor_slices((df_val['pth'],df_val['pth_mask'])).map(get_image,num_parallel_calls=AUTOTUNE).\\\n","#                         batch(batch_size,drop_remainder=True)"]},{"cell_type":"markdown","metadata":{},"source":["## Objective"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2023-04-15T09:11:03.253398Z","iopub.status.busy":"2023-04-15T09:11:03.252804Z","iopub.status.idle":"2023-04-15T09:11:07.339180Z","shell.execute_reply":"2023-04-15T09:11:07.338359Z","shell.execute_reply.started":"2023-04-15T09:11:03.253362Z"},"trusted":true},"outputs":[],"source":["vgg=VGG()\n"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2023-04-15T09:11:07.342979Z","iopub.status.busy":"2023-04-15T09:11:07.342764Z","iopub.status.idle":"2023-04-15T09:11:07.354857Z","shell.execute_reply":"2023-04-15T09:11:07.354088Z","shell.execute_reply.started":"2023-04-15T09:11:07.342950Z"},"trusted":true},"outputs":[],"source":["def l1(y_true, y_pred):\n","    if K.ndim(y_true) == 4:\n","        return K.mean(K.abs(y_pred - y_true), axis=[1,2,3])\n","    elif K.ndim(y_true) == 3:\n","        return K.mean(K.abs(y_pred - y_true), axis=[1,2])\n","    \n","    \n","def gram_matrix(x):\n","    # Permute channels and get resulting shape\n","    x = tf.transpose(x, perm=(0, 3, 1, 2))\n","    shape = tf.shape(x)\n","    B, C, H, W = shape[0], shape[1], shape[2], shape[3]\n","        \n","    # Reshape x and do batch dot product\n","    features = tf.reshape(x, tf.stack([B, C, H*W]))\n","    gram = tf.keras.backend.batch_dot(features, features, axes=2)\n","        \n","    # Normalize with channels, height and width\n","    gram = gram /  tf.cast(C * H * W, x.dtype)\n","        \n","    return gram"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2023-04-15T09:11:07.356178Z","iopub.status.busy":"2023-04-15T09:11:07.355923Z","iopub.status.idle":"2023-04-15T09:11:07.370901Z","shell.execute_reply":"2023-04-15T09:11:07.370112Z","shell.execute_reply.started":"2023-04-15T09:11:07.356141Z"},"trusted":true},"outputs":[],"source":["\n","def loss_hole(mask, y_true, y_pred):\n","    return l1((1-mask) * y_true, (1-mask) * y_pred)\n","    \n","def loss_valid(mask, y_true, y_pred):\n","    return l1(mask * y_true, mask * y_pred)\n","    \n","def loss_perceptual(vgg_out, vgg_gt, vgg_comp): \n","    loss = 0\n","    for o, c, g in zip(vgg_out, vgg_comp, vgg_gt):\n","        loss += l1(o, g) + l1(c, g)\n","    return loss\n","        \n","def loss_style(output, vgg_gt):\n","    loss = 0\n","    for o, g in zip(output, vgg_gt):\n","        loss += l1(gram_matrix(o), gram_matrix(g))\n","    return loss\n","\n","    \n","def loss_tv(mask, y_comp):\n","    kernel = tf.ones(shape=(3, 3, mask.shape[3], mask.shape[3]))\n","    dilated_mask = K.conv2d(1-mask, kernel, data_format='channels_last', padding='same')\n","\n","    dilated_mask = tf.cast(K.greater(dilated_mask, 0), 'float32')\n","    P = dilated_mask * y_comp\n","\n","    a = l1(P[:,1:,:,:], P[:,:-1,:,:])\n","    b = l1(P[:,:,1:,:], P[:,:,:-1,:])        \n","    return a+b"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2023-04-15T09:11:07.373642Z","iopub.status.busy":"2023-04-15T09:11:07.372879Z","iopub.status.idle":"2023-04-15T09:11:07.381379Z","shell.execute_reply":"2023-04-15T09:11:07.380500Z","shell.execute_reply.started":"2023-04-15T09:11:07.373587Z"},"trusted":true},"outputs":[],"source":["mean = [0.485, 0.456, 0.406]\n","std = [0.229, 0.224, 0.225]\n","\n","def loss_func(y_true, y_pred, mask):\n","    y_comp = mask * y_true + (1-mask) * y_pred\n","    vgg_out = vgg((y_pred-mean)/std)\n","    vgg_gt = vgg((y_true-mean)/std)\n","    vgg_comp = vgg((y_comp-mean)/std)\n","            \n","    l1 = loss_valid(mask, y_true, y_pred)\n","    l2 = loss_hole(mask, y_true, y_pred)\n","    l3 = loss_perceptual(vgg_out, vgg_gt, vgg_comp)\n","    l4 = loss_style(vgg_out, vgg_gt)\n","    l5 = loss_style(vgg_comp, vgg_gt)\n","    l6 = loss_tv(mask, y_comp)\n","    \n","    return l1 + 6*l2 + 0.05*l3 + 120*(l4+l5) + 0.1*l6"]},{"cell_type":"markdown","metadata":{},"source":["## Train"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2023-04-15T09:11:08.209302Z","iopub.status.busy":"2023-04-15T09:11:08.208457Z","iopub.status.idle":"2023-04-15T09:11:08.218144Z","shell.execute_reply":"2023-04-15T09:11:08.217349Z","shell.execute_reply.started":"2023-04-15T09:11:08.209148Z"},"trusted":true},"outputs":[],"source":["def show(x,mask,model,n=6):\n","    \n","    x_masked= x*mask+(1-mask)\n","    \n","    x_pred=model([x_masked,mask],training=False)\n","    \n","    mask = tf.concat([mask for _ in range(3)], -1)\n","    \n","    fig,ax=plt.subplots(nrows=3,ncols=n,figsize=(8,8))\n","    \n","    for i in range(3):\n","        for j in range(n):\n","            if i==1:\n","                x=x_masked\n","            elif i==2:\n","                x=x_pred\n","            ax[i,j].imshow(x[j])\n","    plt.show()"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2023-04-15T09:11:09.086149Z","iopub.status.busy":"2023-04-15T09:11:09.085417Z","iopub.status.idle":"2023-04-15T09:11:09.092088Z","shell.execute_reply":"2023-04-15T09:11:09.090999Z","shell.execute_reply.started":"2023-04-15T09:11:09.086110Z"},"trusted":true},"outputs":[],"source":["@tf.function\n","def train_step(x,mask,model,opt):\n","    with tf.GradientTape() as tape:\n","        x_masked= x*mask+(1-mask)\n","        \n","        x_prime=model([x,mask],training=True)\n","        \n","        loss=tf.reduce_mean(loss_func(x,x_prime,mask))\n","        \n","    grad=tape.gradient(loss,model.trainable_variables)\n","    opt.apply_gradients(zip(grad,model.trainable_variables))\n","    \n","    return loss"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2023-04-15T09:11:09.729764Z","iopub.status.busy":"2023-04-15T09:11:09.729153Z","iopub.status.idle":"2023-04-15T09:11:09.737335Z","shell.execute_reply":"2023-04-15T09:11:09.736600Z","shell.execute_reply.started":"2023-04-15T09:11:09.729728Z"},"trusted":true},"outputs":[],"source":["def train(learning_rate = 1e-5):\n","    try:\n","        G=tf.keras.models.load_model('../input/inpainting-models/model_bn/model_in')\n","        \n","        if fine_tune:\n","            for i in frozen_layers:\n","                G.layers[i].trainable=False\n","                lr=1e-5\n","    except:\n","        G=generator('In')\n","    lr = learning_rate\n","    optimizer=tf.keras.optimizers.Adam(learning_rate=lr)\n","    ckpt = tf.train.Checkpoint(G=G,optimizer=optimizer)\n","    ckpt_manager = tf.train.CheckpointManager(ckpt,'./ckpt', max_to_keep=1)\n","    print('start training')\n","    for epoch in range(epochs):\n","        if epoch%5==0:\n","            print('sampling')\n","#             for x,mask in ds_val:\n","#                 show(x,mask,G)\n","#                 break \n","                \n","            #save\n","            ckpt_manager.save()\n","            tf.keras.models.save_model(G,'./model')\n","            \n","        loop=tqdm(ds_train)\n","        for x,mask in loop:\n","            loss=train_step(x,mask,G,optimizer)\n","            loop.set_postfix(loss=f'loss:{loss}')\n","    return G"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2023-04-15T09:11:10.939159Z","iopub.status.busy":"2023-04-15T09:11:10.938857Z","iopub.status.idle":"2023-04-15T09:12:02.584376Z","shell.execute_reply":"2023-04-15T09:12:02.583302Z","shell.execute_reply.started":"2023-04-15T09:11:10.939117Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["start training\n","sampling\n"]},{"name":"stderr","output_type":"stream","text":["  2%|▏         | 26/1312 [00:40<33:45,  1.58s/it, loss=loss:8.809263229370117]  \n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_17/319435375.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mG\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_17/3636685513.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(learning_rate)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmask\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_postfix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf'loss:{loss}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__format__\u001b[0;34m(self, format_spec)\u001b[0m\n\u001b[1;32m   1010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1011\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__format__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1012\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__format__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1013\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1058\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1060\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1061\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m       \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":[" \n","G=train()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-04-15T09:09:31.979970Z","iopub.status.idle":"2023-04-15T09:09:31.980604Z","shell.execute_reply":"2023-04-15T09:09:31.980382Z","shell.execute_reply.started":"2023-04-15T09:09:31.980357Z"},"trusted":true},"outputs":[],"source":["import tensorflow as tf\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import cv2\n","\n","img_path = 'Dataset/Testing_Data/Cat-Train (1).png'  # path to test image\n","img = cv2.imread(img_path)  # load image using OpenCV\n","# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # convert to RGB format\n","\n","# Preprocess the image by normalizing pixel values and resizing\n","img = cv2.resize(img, (256, 256))  # resize to match model input shape\n","img = img / 255.0  # normalize pixel values to range [0, 1]\n","img = np.expand_dims(img, axis=0)  # add batch dimension\n","model = G\n","# Use the model to generate the inpainted image\n","inpainted_img = model.predict([img,img])\n","# Convert the inpainted image from tensor to numpy array\n","inpainted_img = inpainted_img[0]  # remove batch dimension\n","print(inpainted_img[27][0][1])\n","\n","# inpainted_img = inpainted_img * 255.0  # rescale pixel values to range [0, 255]\n","# inpainted_img = inpainted_img.astype('uint8')  # convert to integer type\n","# inpainted_img = cv2.cvtColor(inpainted_img, cv2.COLOR_RGB2BGR)  # convert to BGR format\n","\n","# Display the original and inpainted images side by side\n","# fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n","# ax[0].imshow(cv2.cvtColor(img[0], cv2.COLOR_RGB2BGR))\n","# ax[0].set_title('Original Image')\n","# ax[1].imshow(inpainted_img)\n","# ax[1].set_title('Inpainted Image')\n","# plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-04-15T09:09:31.981955Z","iopub.status.idle":"2023-04-15T09:09:31.982577Z","shell.execute_reply":"2023-04-15T09:09:31.982358Z","shell.execute_reply.started":"2023-04-15T09:09:31.982334Z"},"trusted":true},"outputs":[],"source":["content_dir = \"Dataset/Testing_Data/\"\n","mask_dir = \"/kaggle/working/mask_files\"\n","gen_dir = \"/kaggle/working/gen_files\"\n","sub_dir = \"submission\"\n","metadata = pd.read_csv(content_dir + \"masked_info.csv\")\n","f = open(\"Final_csv_file.csv\",\"w\")\n","f.write(\"filename_box_pixel,Value\\n\")\n","# f = open(sub_dir+\"/DL_Group28_Sub1.csv\",\"w\")\n","vals_dict = {\n","    'filename_box_pixel' : [],\n","    'Value' : []\n","}\n","print('meta'data.shape[0])\n","\n","\n","# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # convert to RGB format\n","\n","\n","model = G\n","\n","# print(inpainted_img)\n","\n","for idx in range(metadata.shape[0]):\n","    _, filename, mask1_y, mask1_x, mask2_y, mask2_x = metadata.iloc[idx,:].tolist()\n","    cords_box1 = [(row, col) for row in range(mask1_y,  mask1_y + 75) for col in range(mask1_x,  mask1_x + 75)]\n","    cords_box2 = [(row, col) for row in range(mask2_y,  mask2_y + 75) for col in range(mask2_x,  mask2_x + 75)]\n","    box_cords = {'box1' : cords_box1, 'box2' : cords_box2}\n","\n","#     gen_filename = gen_dir + \"/\" + filename[0:filename.find(\".\")] + '.jpg'\n","\n","#     gen_img = Image.open(gen_filename)\n","    img_path = 'Dataset/Testing_Data/'+filename  # path to test image\n","    img = cv2.imread(img_path)  # load image using OpenCV\n","    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # convert to RGB format\n","    \n","        # Preprocess the image by normalizing pixel values and resizing\n","    img = cv2.resize(img, (256, 256))  # resize to match model input shape\n","    img = img / 255.0  # normalize pixel values to range [0, 1]\n","    img = np.expand_dims(img, axis=0)  # add batch dimension\n","        # Use the model to generate the inpainted image\n","    inpainted_img = model.predict([img,img])\n","    # Convert the inpainted image from tensor to numpy array\n","    test_results = inpainted_img[0]  # remove batch dimension\n","    for box, all_cords in box_cords.items():\n","        for px_cords in all_cords:\n","            b = test_results[px_cords[0]][px_cords[1]][0]\n","            g = test_results[px_cords[0]][px_cords[1]][1]\n","            r = test_results[px_cords[0]][px_cords[1]][2]\n","#             r, g, b = gen_img.getpixel((px_cords[0], px_cords[1]))\n","#             print(b)\n","            px_name_b = filename + \"_\" + box + \"_\" + str(px_cords[0]) + \"_\" + str(px_cords[1]) + \"_0\"\n","            px_name_g = filename + \"_\" + box + \"_\" + str(px_cords[0]) + \"_\" + str(px_cords[1]) + \"_1\"\n","            px_name_r = filename + \"_\" + box + \"_\" + str(px_cords[0]) + \"_\" + str(px_cords[1]) + \"_2\"\n","#             print(px_name_b+\",\"+str(b))\n","#             b = (b+1)/2\n","#             g = (g+1)/2\n","#             r = (r+1)/2\n","            f.write(px_name_b+\",\"+str(b)+\"\\n\")\n","            f.write(px_name_g+\",\"+str(g)+\"\\n\")\n","            f.write(px_name_r+\",\"+str(r)+\"\\n\")\n","#             vals_dict['filename_box_pixel'].extend([px_name_b, px_name_g, px_name_r])\n","#             vals_dict['Value'].extend([b, g, r])\n","\n","    if idx % 5 == 0:\n","        print(idx)\n","f.close()\n","# print('Lists Created')\n","# sub_df = pd.DataFrame(vals_dict)\n","# print('DF Created')\n","# # sub_df['Value'] = sub_df['Value'] / 255\n","# # print('Score Normalised')\n","# sub_df.to_csv(sub_dir+\"/\"'DL_Group28_Sub1.csv', index=False)\n","# print('CSV Generated')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
